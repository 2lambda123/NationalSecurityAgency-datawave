CONFIGURATION=test
RCPT_TO=hadoop@localhost

# ingest properties 
WAREHOUSE_ACCUMULO_HOME=/local/accumulo-current
WAREHOUSE_HDFS_NAME_NODE=hdfs://localhost:9000
WAREHOUSE_JOBTRACKER_NODE=localhost:8032
WAREHOUSE_ZOOKEEPERS=localhost:2181
WAREHOUSE_INSTANCE_NAME=SuperStar
#Sets variable sets the zookeeper location for the warehouse side
zookeeper.hosts=localhost:2181

INGEST_ACCUMULO_HOME=/local/accumulo-current
INGEST_HDFS_NAME_NODE=hdfs://localhost:9000
INGEST_JOBTRACKER_NODE=localhost:8032
INGEST_ZOOKEEPERS=localhost:2181
INGEST_INSTANCE_NAME=SuperStar

STAGING_HOSTS=localhost
DUMPER_HOSTS=localhost
INGEST_HOST=localhost
ROLLUP_HOST=localhost

#extra mapreduce options (e.g. mapreduce.task.io.sort.mb and the like)
MAPRED_INGEST_OPTS=-useInlineCombiner

#extra HADOOP_OPTS (java options)
HADOOP_INGEST_OPTS=

#extra CHILD_OPTS (java options)
CHILD_INGEST_OPTS= 

ONE_HR_CHILD_MAP_MAX_MEMORY_MB=2048
FIVE_MIN_CHILD_MAP_MAX_MEMORY_MB=1024
FIFTEEN_MIN_CHILD_MAP_MAX_MEMORY_MB=1024
ONE_HR_CHILD_REDUCE_MAX_MEMORY_MB=2048
FIVE_MIN_CHILD_REDUCE_MAX_MEMORY_MB=1024
FIFTEEN_MIN_CHILD_REDUCE_MAX_MEMORY_MB=1024

# the next three variables work in concert with each other and must align
# (i.e. the first of the POLLER_DATA_TYPES pulls from the first of the POLLER_INPUT_DIRECTORIES and
#  outputs into the first of the POLLER_OUTPUT_DIRECTORIES using the first of the POLLER_CLIENT_OPTS)
POLLER_DATA_TYPES=mycsv
POLLER_INPUT_DIRECTORIES=/local/pollerinput
POLLER_OUTPUT_DIRECTORIES=/local/polleroutput/mycsv
POLLER_CLIENT_OPTS=,,
POLLER_FILE_BLOCK_SIZE_MB=5
POLLER_FIVE_MIN_FILE_BLOCK_SIZE_MB=5
POLLER_FIFTEEN_MIN_FILE_BLOCK_SIZE_MB=5
POLLER_ONE_HR_FILE_BLOCK_SIZE_MB=5
POLLER_JMX_PORT_START=20040

ONE_HR_INGEST_DATA_TYPES=mycsv
FIFTEEN_MIN_INGEST_DATA_TYPES=
FIVE_MIN_INGEST_DATA_TYPES=wikipedia
BULK_INGEST_DATA_TYPES=
LIVE_INGEST_DATA_TYPES=

# Clear out these values if you do not want standard shard ingest.
DEFAULT_SHARD_HANDLER_CLASSES=nsa.datawave.ingest.mapreduce.handler.shard.AbstractColumnBasedHandler
ALL_HANDLER_CLASSES=nsa.datawave.ingest.mapreduce.handler.edge.ProtobufEdgeDataTypeHandler,nsa.datawave.ingest.mapreduce.handler.dateindex.DateIndexDataTypeHandler

ONE_HR_INGEST_REDUCERS=10
FIFTEEN_MIN_INGEST_REDUCERS=10
FIVE_MIN_INGEST_REDUCERS=10

# Note the max blocks per job must be less than or equal to the number of mappers
INGEST_ONE_HR_JOBS=1
INGEST_ONE_HR_MAPPERS=4
INGEST_MAX_ONE_HR_BLOCKS_PER_JOB=4
INGEST_FIFTEEN_MIN_JOBS=1
INGEST_FIFTEEN_MIN_MAPPERS=4
INGEST_MAX_FIFTEEN_MIN_BLOCKS_PER_JOB=4
INGEST_FIVE_MIN_JOBS=1
INGEST_FIVE_MIN_MAPPERS=4
INGEST_MAX_FIVE_MIN_BLOCKS_PER_JOB=4

INDEX_STATS_MAX_MAPPERS=7

POLLER_THREADS=3

NUM_MAP_LOADERS=1

USERNAME=root
PASSWORD=secret

ZOOKEEPER_HOME=/local/zookeeper
HADOOP_HOME=/local/hadoop
MAPRED_HOME=/local/hadoop

WAREHOUSE_HADOOP_CONF=/local/hadoop/conf
INGEST_HADOOP_CONF=/local/hadoop/conf

HDFS_BASE_DIR=/NewIngest

MONITOR_SERVER_HOST=localhost

LOG_DIR=/local/logs/ingest
FLAG_DIR=/local/data/flags
FLAG_MAKER_CONFIG=/local/dw/config/FiveMinFlagMaker.xml,/local/dw/config/OneHrFlagMaker.xml,/local/dw/config/WikipediaFlagMakerConfig.xml
BIN_DIR_FOR_FLAGS=/local/dw/bin

NUM_SHARDS=10

PYTHON=/usr/bin/python

# Two years: 84600 * 365 * 2 * 1000
EVENT_DISCARD_INTERVAL=63072000000

DATAWAVE_CACHE_PORT=20444
SHARD_TABLE_NAME=shard
SHARD_INDEX_TABLE_NAME=shardIndex
SHARD_REVERSE_INDEX_TABLE_NAME=shardReverseIndex
METADATA_TABLE_NAME=DatawaveMetadata
EDGE_TABLE_NAME=edge
SHARD_INDEX_STATS_TABLE_NAME=shardIndexStats
PROTOBUF_EDGE_TABLE_NAME=protobufedge
ALL_PAIRS_INDEX_TABLE_NAME=allPairsIndex
ERROR_METADATA_TABLE_NAME=errorMetadata
ERROR_SHARD_INDEX_TABLE_NAME=errorIndex
ERROR_SHARD_REVERSE_INDEX_TABLE_NAME=errorReverseIndex
ERROR_SHARD_TABLE_NAME=errorShard
KNOWLEDGE_METADATA_TABLE_NAME=knowledgeMetadata
KNOWLEDGE_SHARD_INDEX_TABLE_NAME=knowledgeIndex
KNOWLEDGE_SHARD_REVERSE_INDEX_TABLE_NAME=knowledgeReverseIndex
KNOWLEDGE_SHARD_TABLE_NAME=knowledgeShard
KNOWLEDGE_TABLE_NAME=knowledge
PROCESSING_ERRORS_TABLE_NAME=processingErrors
QUERY_METRICS_BASE_NAME=QueryMetrics

ERROR_TABLE=errors
ANALYTIC_MTX=analytic_metrics
LOADER_MTX=loader_metrics
POLLER_MTX=poller_metrics
INGEST_MTX=ingest_metrics
BULK_INGEST_METRIC_THRESHOLD=1500000
LIVE_INGEST_METRIC_THRESHOLD=1500000

KEYSTORE=/local/wildfly/standalone/configuration/certificates/testServer.p12
KEYSTORE_TYPE=PKCS12
KEYSTORE_PASSWORD=secret
TRUSTSTORE=/local/wildfly-9.0.0.Final/standalone/configuration/certificates/ca.jks

FLAG_METRICS_DIR=/data/flagMetrics
TRUSTSTORE_PASSWORD=Changeit1
TRUSTSTORE_TYPE=JKS

cluster.name=dev
accumulo.instance.name=SuperStar
accumulo.user.name=root
accumulo.user.password=secret
cached.results.hdfs.uri=hdfs://localhost:8020/
cached.results.export.dir=/CachedResults

type.metadata.hdfs.uri=hdfs://localhost:8020/
type.metadata.dir=/TypeMetadata
type.metadata.fileName=typeMetadata

lock.file.dir=/tmp/poller/
JAVA_HOME=/local/jdk1.7.0_79

# query properties
server.keystore.password=secret
accumulo.user.password=secret
mysql.user.password=datawave
jboss.jmx.password=blah
hornetq.cluster.password=blah
hornetq.system.password=blah

server.truststore.password=Changeit
accumulo.instance.name=SuperStar
cluster.name=SuperStar

#Sets up the Atom Service
atom.wildfly.hostname=localhost
atom.wildfly.port.number=8443
atom.connection.pool.name=WAREHOUSE


security.use.testauthservice=true
security.testauthservice.context.entry=<value>classpath*:datawave/security/TestAuthorizationServiceConfiguration.xml</value>
security.testauthservice.entries= \
\n	<entry key="cn=test a. user, ou=my department, o=my company, st=some-state, c=us"> \
\n 		<list> \
\n <value>AuthorizedUser</value><value>PRIVATE</value><value>PUBLIC</value><value>Administrator</value> \
\n		</list> \
\n	</entry>

# other properties
rpm.file.owner=rpmowner
rpm.file.group=rpmowner
rpm.file.accumulo.owner=accumulo-owner
rpm.file.accumulo.group=accumulo-owner

# Enable full table scans for the base event query?
#beq.fullTableScanEnabled=true

event.query.data.decorators= \
          <entry key="MY_URL"> \
\n            <bean class="nsa.datawave.query.transformer.EventQueryDataDecorator"> \
\n                <property name="fieldName" value="MY_URL"/> \
\n                <property name="patternMap"> \
\n                    <map key-type="java.lang.String" value-type="java.lang.String"> \
\n                        <entry key="EVENT_ID" value="https://localhost:8443/DataWave/Query/lookupUUID/EVENT_ID?uuid=@field_value@&amp;parameters=data.decorators:MY_URL"/> \
\n                        <entry key="UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/UUID?uuid=@field_value@&amp;parameters=data.decorators:MY_URL"/> \
\n                        <entry key="PARENT_UUID" value="https://localhost:8443/DataWave/Query/lookupUUID/PARENT_UUID?uuid=@field_value@&amp;parameters=data.decorators:MY_URL"/> \
\n                    </map> \
\n                </property> \
\n            </bean> \
\n        </entry>

lookup.uuid.uuidTypes= \
          <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="EVENT_ID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n            <property name="allowWildcardAfter" value="28" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="UUID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="PARENT_UUID" /> \
\n            <property name="definedView" value="LuceneUUIDEventQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="event" /> \
\n            <property name="definedView" value="LookupUIDQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="children" /> \
\n            <property name="definedView" value="LookupUIDQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="parent" /> \
\n            <property name="definedView" value="LookupUIDQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="tree" /> \
\n            <property name="definedView" value="LookupUIDQuery" /> \
\n        </bean> \
\n        <bean class="nsa.datawave.query.data.UUIDType"> \
\n            <property name="fieldName" value="descendents" /> \
\n            <property name="definedView" value="LookupUIDQuery" /> \
\n        </bean>

query.metrics.marking=(PUBLIC)
query.metrics.visibility=PUBLIC

metrics.warehouse.namenode=localhost
metrics.warehouse.hadoop.path=/local/hadoop

